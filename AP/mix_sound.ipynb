{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.csdn.net/sinat_28442665/article/details/84029633\n",
    "Show how to check the information about WAV\n",
    "https://blog.csdn.net/qq_38563206/article/details/126486544\n",
    "Show how to Mixing, overlaying audio, splicing audio, here we need to do overlaying audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So they use FSD50K, ESC-50, MUSDB18, DISCO\n",
    "https://research.google.com/audioset/download.html\n",
    "\n",
    "Starting from 2 sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from FSD50K\n",
    "datasetpath = '../data/BinauralCuratedDataset/FSD50K/'\n",
    "ds_dev_path = datasetpath + 'FSD50K.dev_audio/'\n",
    "ds_dev_gt_path = datasetpath + 'FSD50K.ground_truth/dev.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read labels grom gt, and sort\n",
    "# import block\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40966it [00:48, 837.68it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40966, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get labels and seperate into single labels.\n",
    "# print(os.path.exists('../data/BinauralCuratedDataset/FSD50K/FSD50K.ground_truth/dev.csv'))\n",
    "df_dev_gt = pd.read_csv(ds_dev_gt_path)\n",
    "df_dev_gt = df_dev_gt.get(['fname', 'labels'])\n",
    "df_dev_gt_sep = pd.DataFrame(columns=['fname', 'label'])\n",
    "for i, v in tqdm(enumerate(df_dev_gt.index)):\n",
    "    t1 = df_dev_gt['labels'][i].split(',')\n",
    "    t2 = df_dev_gt['fname'][i]\n",
    "    l = len(t1)\n",
    "    for i2, v2 in enumerate(t1):\n",
    "        df_dev_gt_sep = pd.concat([df_dev_gt_sep, pd.DataFrame([[t2,v2]], columns=df_dev_gt_sep.columns)], ignore_index=True)\n",
    "df_dev_gt.shape\n",
    "# this may have a problem because some classes are too closed, for example keyboard and piano. It is a muti-label prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40966, 2)\n",
      "(114271, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df_dev_gt.shape)\n",
    "print(df_dev_gt_sep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40966it [01:30, 453.92it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73198, 6)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a two sound mixture.\n",
    "# select two data from the df_dev_gt data, where they dont share any same label. \n",
    "# if they have multiple labels, we choose a random one.\n",
    "def check_share_attribute(s1, s2):\n",
    "    l1 = s1.split(',')\n",
    "    l2 = s2.split(',')\n",
    "    if list(set(l1)&set(l2)):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "np.random.seed(66)\n",
    "def return_drop_rate(rate):\n",
    "    return np.random.random() > rate\n",
    "\n",
    "df_dev_mix_no_share = pd.DataFrame(columns=['newfname', 'fname1', 'fname2', 'labels', 'gt_label', 'gt_index'])\n",
    "# let's make a some mixtures first\n",
    "for i, v in tqdm(enumerate(df_dev_gt.index)):\n",
    "    if not return_drop_rate(0.99):\n",
    "            continue\n",
    "    for i2, v2 in enumerate(df_dev_gt.index):\n",
    "        if not return_drop_rate(0.99):\n",
    "            continue\n",
    "        if i >= i2:\n",
    "            continue\n",
    "        l1 = df_dev_gt['labels'][i]\n",
    "        l2 = df_dev_gt['labels'][i2]\n",
    "        r1 = np.random.randint(0, len(l1.split(',')))\n",
    "        r2 = np.random.randint(0, len(l2.split(',')))\n",
    "        l1 = l1.split(',')[r1]\n",
    "        l2 = l2.split(',')[r2]\n",
    "        r3 = np.random.randint(0, 2)\n",
    "        if check_share_attribute(df_dev_gt['labels'][i], df_dev_gt['labels'][i2]):\n",
    "            continue\n",
    "        else:\n",
    "            newrow = [str(df_dev_gt['fname'][i])+'_'+str(df_dev_gt['fname'][i2]), str(df_dev_gt['fname'][i]), str(df_dev_gt['fname'][i2]), l1+','+l2, [l1,l2][r3], r3]\n",
    "            df_dev_mix_no_share = pd.concat([df_dev_mix_no_share, pd.DataFrame([newrow], columns=df_dev_mix_no_share.columns)], ignore_index=True)\n",
    "df_dev_mix_no_share.shape\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly use 10k data for small experiement:\n",
    "small_box_experiment = np.random.choice(df_dev_mix_no_share.index, 10000, replace=False)\n",
    "df_dev_mix_no_share_small = df_dev_mix_no_share.loc[small_box_experiment]\n",
    "\n",
    "small_box_test=df_dev_mix_no_share_small.sample(frac=0.2)\n",
    "t_df=df_dev_mix_no_share_small[~df_dev_mix_no_share_small.index.isin(small_box_test.index)]\n",
    "small_box_val=t_df.sample(frac=0.25)\n",
    "small_box_train=t_df[~t_df.index.isin(small_box_val.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6000it [00:07, 802.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# make dataset for the small experiment\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "wav_path = '../data/BinauralCuratedDataset/FSD50K/FSD50K.dev_audio/'\n",
    "for i, wav_t in tqdm(small_box_train.iterrows()):\n",
    "    sound1 = AudioSegment.from_wav(wav_path+wav_t['fname1']+'.wav')\n",
    "    sound2 = AudioSegment.from_wav(wav_path+wav_t['fname2']+'.wav')\n",
    "    if len(sound1) > len(sound2):\n",
    "        output = sound1.overlay(sound2) # overlay them\n",
    "    else:\n",
    "        output = sound2.overlay(sound1)\n",
    "    output.export('./data_folder/train/'+wav_t['newfname']+'.wav', format=\"wav\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:04, 419.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# make dataset for the small experiment\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "wav_path = '../data/BinauralCuratedDataset/FSD50K/FSD50K.dev_audio/'\n",
    "for i, wav_t in tqdm(small_box_val.iterrows()):\n",
    "    sound1 = AudioSegment.from_wav(wav_path+wav_t['fname1']+'.wav')\n",
    "    sound2 = AudioSegment.from_wav(wav_path+wav_t['fname2']+'.wav')\n",
    "    if len(sound1) > len(sound2):\n",
    "        output = sound1.overlay(sound2) # overlay them\n",
    "    else:\n",
    "        output = sound2.overlay(sound1)\n",
    "    output.export('./data_folder/val/'+wav_t['newfname']+'.wav', format=\"wav\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:07, 274.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# make dataset for the small experiment\n",
    "from pydub import AudioSegment\n",
    "import librosa\n",
    "\n",
    "wav_path = '../data/BinauralCuratedDataset/FSD50K/FSD50K.dev_audio/'\n",
    "for i, wav_t in tqdm(small_box_test.iterrows()):\n",
    "    sound1 = AudioSegment.from_wav(wav_path+wav_t['fname1']+'.wav')\n",
    "    sound2 = AudioSegment.from_wav(wav_path+wav_t['fname2']+'.wav')\n",
    "    if len(sound1) > len(sound2):\n",
    "        output = sound1.overlay(sound2) # overlay them\n",
    "    else:\n",
    "        output = sound2.overlay(sound1)\n",
    "    output.export('./data_folder/test/'+wav_t['newfname']+'.wav', format=\"wav\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store \n",
    "df_dev_mix_no_share_small.to_csv('data_folder/df_dev_mix_no_share_small.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hearing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
